# IMPORTANT NOTE: The code structure was generated by Cursor here, it entails a program that communicates
# with the OpenAI API to generate responses. The prompts were completely originated from me.
import openai
from typing import List, Dict, Tuple, Generator
import json

class AIService:
    def __init__(self, api_key: str):
        self.client = openai.OpenAI(api_key=api_key)
        self.api_key = api_key  # Store for debugging (masked)
        self.response_model = "gpt-4o" # Model for chat responses (reasoning model)
        self.feedback_model = "gpt-4o" # Model for feedback generation
    
    def _mask_api_key(self, api_key: str) -> str:
        """Mask API key for logging - shows first 7 and last 4 characters"""
        if not api_key or len(api_key) < 12:
            return "***INVALID***"
        return f"{api_key[:7]}...{api_key[-4:]}"
    
    def _is_temperature_restricted_model(self, model: str) -> bool:
        """Check if the model only supports temperature=1.0 (default) - o1 models and gpt-5 models"""
        temperature_restricted = ['o1', 'o1-preview', 'o1-mini', 'gpt-5']
        model_lower = model.lower()
        return any(restricted_model in model_lower for restricted_model in temperature_restricted)
    
    def _supports_streaming(self, model: str) -> bool:
        """Check if the model supports streaming - o1 models don't support streaming, but gpt-5 models do"""
        # o1 models don't support streaming
        non_streaming_models = ['o1', 'o1-preview', 'o1-mini']
        model_lower = model.lower()
        return not any(non_streaming_model in model_lower for non_streaming_model in non_streaming_models)
    
    def _extract_pdf_text(self, base64_data: str) -> str:
        """Extract text content from a PDF file"""
        try:
            import base64
            import io
            from PyPDF2 import PdfReader
            
            # Decode base64 to bytes
            pdf_bytes = base64.b64decode(base64_data)
            pdf_stream = io.BytesIO(pdf_bytes)
            
            # Read PDF
            pdf_reader = PdfReader(pdf_stream)
            text_content = []
            
            # Extract text from each page
            for page_num, page in enumerate(pdf_reader.pages):
                page_text = page.extract_text()
                if page_text.strip():
                    text_content.append(f"--- Page {page_num + 1} ---\n{page_text}")
            
            extracted_text = "\n\n".join(text_content)
            return extracted_text
        except ImportError:
            print("ERROR: PyPDF2 not installed. Install with: pip install PyPDF2")
            return "[PDF content could not be extracted - PyPDF2 library not installed]"
        except Exception as e:
            print(f"ERROR: Failed to extract PDF text: {e}")
            import traceback
            print(traceback.format_exc())
            return f"[PDF content could not be extracted: {str(e)}]"
    
    def _format_message_with_attachments(self, msg: Dict) -> Dict:
        """Format a message for OpenAI API, handling text, image, and PDF attachments"""
        role = msg.get('role')
        content = msg.get('content', '')
        attachments = msg.get('attachments', [])
        
        # Separate image and PDF attachments
        image_attachments = [att for att in attachments if att.get('data') and att.get('file_type', '').startswith('image/')]
        pdf_attachments = [
            att for att in attachments 
            if att.get('data') and (
                att.get('file_type', '') == 'application/pdf' or
                att.get('filename', '').lower().endswith('.pdf')
            )
        ]
        
        # Extract text from PDFs and append to content
        pdf_texts = []
        for pdf_att in pdf_attachments:
            pdf_text = self._extract_pdf_text(pdf_att.get('data'))
            pdf_filename = pdf_att.get('filename', 'document.pdf')
            pdf_texts.append(f"[Content from {pdf_filename}]\n{pdf_text}")
        
        # Combine original content with PDF text
        combined_content = content
        if pdf_texts:
            combined_content = (content + "\n\n" + "\n\n".join(pdf_texts)).strip() if content else "\n\n".join(pdf_texts)
        
        # If there are image attachments, format for vision API
        if image_attachments:
            # Format as content array for vision API
            content_array = []
            
            # Add text content if present (including PDF-extracted text)
            if combined_content and combined_content.strip():
                content_array.append({
                    "type": "text",
                    "text": str(combined_content)
                })
            
            # Add image attachments
            for att in image_attachments:
                base64_data = att.get('data')
                file_type = att.get('file_type', 'image/jpeg')
                # Format as data URL for OpenAI
                data_url = f"data:{file_type};base64,{base64_data}"
                content_array.append({
                    "type": "image_url",
                    "image_url": {
                        "url": data_url
                    }
                })
            
            return {
                "role": role,
                "content": content_array
            }
        elif pdf_attachments:
            # PDFs but no images - return text message with extracted PDF content
            return {
                "role": role,
                "content": combined_content if combined_content else "[PDF content could not be extracted]"
            }
        else:
            # Regular text message
            return {
                "role": role,
                "content": str(content) if content else ""
            }
    
    def get_chat_response(self, messages: List[Dict], quality_score: float, user_name: str = None) -> str:
        """Get response from the main chat AI based on quality score"""
        
        # this is a personalization
        name_context = f" Address the user as {user_name} once every 5 messages. Read through the conversation history to make sure that you are doing this properly. Don't name the user every single time you respond." if user_name else ""
        
        if quality_score <= 3:
            # really terse, very minimal responses with strong prodding
            system_prompt = f"""You are a helpful AI assistant, but the user's prompt quality is very poor.{name_context}
            Respond in 20-30 words maximum. Be direct and ask for more specific information. 
            Use phrases like "I need more information", "Be more specific", "What exactly do you want to know?"
            Don't provide answers, only ask clarifying questions."""
        elif quality_score <= 5:
            # moderately terse, brief responses with follow-up questions
            system_prompt = f"""You are a helpful AI assistant. The user's prompt quality is below average.{name_context}
            Provide brief responses (50-100 words) and ask follow-up questions to encourage better prompting.
            Ask for more context, specificity, or clarification. Guide them to ask better questions."""
        else:
            # normal helpful responses
            system_prompt = f"""You are a helpful AI assistant.{name_context} Provide clear, accurate, and useful responses to user questions.
            Be thorough and helpful while encouraging good prompting practices."""
        
        try:
            # Ensure messages are properly formatted
            # Filter out any messages that aren't 'user' or 'assistant' role
            # Remove timestamp fields which OpenAI doesn't accept
            formatted_messages = [{"role": "system", "content": system_prompt}]
            for msg in messages:
                if isinstance(msg, dict) and 'role' in msg:
                    role = msg['role']
                    # Only include user and assistant messages (skip system)
                    if role in ['user', 'assistant']:
                        # Use helper function to format messages with attachments
                        formatted_msg = self._format_message_with_attachments(msg)
                        formatted_messages.append(formatted_msg)
            
            if len(formatted_messages) <= 1:  # Only system message
                raise ValueError("No user or assistant messages found in conversation")
            
            # Log detailed debug info
            
            # Check for image attachments
            has_images = any(
                isinstance(msg.get('content'), list) and any(
                    item.get('type') == 'image_url' for item in msg.get('content', [])
                )
                for msg in formatted_messages
            )

            
            # openAI API
            # Some models (o1, gpt-5) only support temperature=1.0 (default), so don't set it
            api_params = {
                "model": self.response_model,
                "messages": formatted_messages
            }
            if not self._is_temperature_restricted_model(self.response_model):
                api_params["temperature"] = 0.7
            
            
            try:
                response = self.client.chat.completions.create(**api_params)
            except Exception as api_error:
                print(f"ERROR: Failed to call OpenAI API: {type(api_error).__name__}: {str(api_error)}")
                raise
            
            # makes sure its going to display properly
            if not response.choices or not response.choices[0].message:
                raise ValueError("Empty response from OpenAI - no choices in response")
            
            content = response.choices[0].message.content
            if not content:
                raise ValueError("Empty content in OpenAI response - message has no content")
            
            return content
        except openai.APIError as e:
            # Handle OpenAI API-specific errors
            error_details = {
                "type": type(e).__name__,
                "message": str(e),
                "status_code": getattr(e, 'status_code', None),
            }
            print(f"ERROR: OpenAI API error: {error_details}")
            import traceback
            print(f"ERROR: Traceback: {traceback.format_exc()}")
            raise
        except openai.RateLimitError as e:
            error_msg = f"OpenAI API rate limit exceeded: {str(e)}"
            print(f"ERROR: {error_msg}")
            raise Exception(f"Rate limit exceeded. Please try again in a moment. Details: {str(e)}")
        except openai.APIConnectionError as e:
            error_msg = f"OpenAI API connection error: {str(e)}"
            print(f"ERROR: {error_msg}")
            raise Exception(f"Failed to connect to OpenAI API. Please check your internet connection. Details: {str(e)}")
        except Exception as e:
            error_type = type(e).__name__
            error_msg = str(e)
            print(f"ERROR: OpenAI API error ({error_type}): {error_msg}")
            import traceback
            print(f"ERROR: Traceback: {traceback.format_exc()}")
            # Re-raise instead of returning error string so we can see the actual error
            raise
    
    def get_chat_response_stream(self, messages: List[Dict], quality_score: float, user_name: str = None) -> Generator[str, None, None]:
        """Get streaming response from the main chat AI based on quality score"""
        
        # this is a personalization
        name_context = f" Address the user as {user_name} once every 5 messages. Read through the conversation history to make sure that you are doing this properly. Don't name the user every single time you respond." if user_name else ""
        
        if quality_score <= 3:
            # really terse, very minimal responses with strong prodding
            system_prompt = f"""You are a helpful AI assistant, but the user's prompt quality is very poor.{name_context}
            Respond in 20-30 words maximum. Be direct and ask for more specific information. 
            Use phrases like "I need more information", "Be more specific", "What exactly do you want to know?"
            Don't provide answers, only ask clarifying questions."""
        elif quality_score <= 5:
            # moderately terse, brief responses with follow-up questions
            system_prompt = f"""You are a helpful AI assistant. The user's prompt quality is below average.{name_context}
            Provide brief responses (50-100 words) and ask follow-up questions to encourage better prompting.
            Ask for more context, specificity, or clarification. Guide them to ask better questions."""
        else:
            # normal helpful responses
            system_prompt = f"""You are a helpful AI assistant.{name_context} Provide clear, accurate, and useful responses to user questions.
            Be thorough and helpful while encouraging good prompting practices."""
        
        try:
            # Format messages for OpenAI API (handling attachments)
            formatted_messages = [{"role": "system", "content": system_prompt}]
            for msg in messages:
                if isinstance(msg, dict) and 'role' in msg:
                    role = msg['role']
                    # Only include user and assistant messages (skip system)
                    if role in ['user', 'assistant']:
                        # Use helper function to format messages with attachments
                        formatted_msg = self._format_message_with_attachments(msg)
                        formatted_messages.append(formatted_msg)
            
            if len(formatted_messages) <= 1:  # Only system message
                raise ValueError("No user or assistant messages found in conversation")
            
            
            # Check for image attachments in messages
            has_images = any(
                isinstance(msg.get('content'), list) and any(
                    item.get('type') == 'image_url' for item in msg.get('content', [])
                )
                for msg in formatted_messages
            )
            
            
            # OpenAI API with streaming
            api_params = {
                "model": self.response_model,
                "messages": formatted_messages,
                "stream": True
            }
            if not self._is_temperature_restricted_model(self.response_model):
                api_params["temperature"] = 0.7
            
            
            try:
                stream = self.client.chat.completions.create(**api_params)
            except Exception as api_error:
                print(f"ERROR: Failed to initiate OpenAI API call: {type(api_error).__name__}: {str(api_error)}")
                raise
            
            chunk_count = 0
            for chunk in stream:
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    if delta and delta.content:
                        chunk_count += 1
                        yield delta.content
        
                        
        except openai.APIError as e:
            # Handle OpenAI API-specific errors
            error_details = {
                "type": type(e).__name__,
                "message": str(e),
                "status_code": getattr(e, 'status_code', None),
                "response": getattr(e, 'response', None)
            }
            print(f"ERROR: OpenAI API error: {error_details}")
            import traceback
            print(f"ERROR: Traceback: {traceback.format_exc()}")
            raise
        except openai.RateLimitError as e:
            error_msg = f"OpenAI API rate limit exceeded: {str(e)}"
            print(f"ERROR: {error_msg}")
            raise Exception(f"Rate limit exceeded. Please try again in a moment. Details: {str(e)}")
        except openai.APIConnectionError as e:
            error_msg = f"OpenAI API connection error: {str(e)}"
            print(f"ERROR: {error_msg}")
            raise Exception(f"Failed to connect to OpenAI API. Please check your internet connection. Details: {str(e)}")
        except Exception as e:
            error_type = type(e).__name__
            error_msg = str(e)
            print(f"ERROR: OpenAI API streaming error ({error_type}): {error_msg}")
            import traceback
            print(f"ERROR: Traceback: {traceback.format_exc()}")
            raise
    
    def _format_messages_for_feedback(self, messages: List[Dict]) -> List[Dict]:
        """Format messages for feedback model, extracting PDF text and noting images"""
        formatted_messages = []
        for msg in messages:
            msg_copy = msg.copy()
            content = msg_copy.get('content', '')
            attachments = msg_copy.get('attachments', [])
            
            # Process attachments
            pdf_texts = []
            image_count = 0
            for att in attachments:
                file_type = att.get('file_type', '')
                filename = att.get('filename', '')
                
                # Extract PDF text
                if (file_type == 'application/pdf' or filename.lower().endswith('.pdf')) and att.get('data'):
                    pdf_text = self._extract_pdf_text(att.get('data'))
                    pdf_texts.append(f"[Content from {filename}]\n{pdf_text}")
                
                # Count images (for now, just note them)
                elif file_type.startswith('image/'):
                    image_count += 1
            
            # Combine content with PDF text
            combined_content = content
            if pdf_texts:
                combined_content = (content + "\n\n" + "\n\n".join(pdf_texts)).strip() if content else "\n\n".join(pdf_texts)
            
            # Note images if present
            if image_count > 0:
                image_note = f"\n\n[User attached {image_count} image file(s)]"
                combined_content = combined_content + image_note if combined_content else f"[User attached {image_count} image file(s)]"
            
            # Remove attachments and update content
            if 'attachments' in msg_copy:
                del msg_copy['attachments']
            msg_copy['content'] = combined_content
            
            formatted_messages.append(msg_copy)
        
        return formatted_messages
    
    def get_feedback_response(self, messages: List[Dict], previous_scores: List[float] = None) -> Tuple[float, str, float]:
        """Get feedback and quality score for the conversation"""
        
        # Format messages to include PDF text and note images
        formatted_messages = self._format_messages_for_feedback(messages)
        
        # Calculate conversation length and context
        conversation_length = len(formatted_messages)
        user_messages = [msg for msg in formatted_messages if msg.get('role') == 'user']
        conversation_depth = len(user_messages)
        
        # Use previous scores for context, default to empty list
        if previous_scores is None:
            previous_scores = []
        
        feedback_prompt = f"""You are an EXTREMELY strict prompt 
        quality assessment AI. Take into account the entire conversation history
         you are given. You MUST give low scores (1-3) for poor conversations. 
         Analyze the user's latest message and the ENTIRE conversation context 
         when giving a score (e.g. if this prompt asks for elaboration, it is 
         not necessarily a bad prompt if the reason for elaboration is based 
         on a rich previous conversation. In the same way, one good prompt does 
         not negate a bad conversation).

CONVERSATION CONTEXT:
- Total messages: {conversation_length}
- User messages: {user_messages}
- Previous scores: {previous_scores}
- This is message #{conversation_depth} in the conversation

IMPORTANT: You MUST be harsh with scoring. Don't be generous. 
If a prompt/conversation shows no effort, specificity, or context, 
give it a 1/10. Most prompts should score 3-7, not 5-8.

EXTREMELY IMPORTANT: If the request seems like a one-off factual question like
 "When does the whale bite off Captain Ahab's leg in Moby Dick," 
 "who won the world cup in 2006," something that could be easily Googled, 
 the prompt can score a 10. This is the most important rule. You cannot 
 throttle fact-based questions. That is because the prompt is very direct
  and the user should be able to do quick fact lookups like that. 
  Otherwise, use this rubric:


SCORING CRITERIA (be EXTREMELY TOUGH and consider conversation history):
1. SPECIFICITY (1-10): How specific and detailed is the request?
   - "summarize this" = 1/10 (too vague, no context)
   - "I don't know" = 1/10 (no specificity at all)
   - "summarize the key findings around copyright infringement from the research paper I shared about AI ethics, with quotes" = 10/10
   - If the user attached a file and is a bit vague but you can understand more of the request based on the image, feel free to score a bit more leniently.

2. CRITICAL THINKING (1-10): Does the prompt show analysis or reasoning?
   - Simple requests that are not well thought through = 2/10
   - Questions that are easy to answer that have DIRECT answers and could easily be googled = 8/10
   - Multi-step questions that the user wants AI to do for them = 2/10
   - Analytical questions = 10/10

3. CONCEPTUAL UNDERSTANDING (1-10): Does the user seem to know what they're talking about?
   - Factual inaccuracies = 1/10
   - The user clearly never having thought of this problem before = 2/10
   - Demonstration of understanding coupled with clear asks of how AI can help = 10/10

4. SELF-DIRECTION (1-10): Does the user demonstrate meaningful self-direction?
   - The user seeming to want AI to do everything for them = 1/10
   - Clear expectations for what the user wants the AI to do and what they will do with the AI's output = 10/10
   - Entire conversation feels like working WITH the user, not FOR the user = 10/10
   - What's very important for this category is clear delineation of "I will use the output for X task" or "you do this, I will do this / I did this / I will follow up with this"
   - The reason this category is here is so that users do not copy-paste detailed assignment instructions without any critical thought on their own part. If it seems like they did that, score them at a 1 no matter what and explain that it seems like they copy-pasted.

FINAL SCORE CALCULATION:
- Average the 4 criteria scores for this message
- Cap at 10.0 maximum
- Round to 1 decimal place

EXAMPLES OF LOW SCORES (be VERY strict):
- "summarize this" (no context, no specificity) = 1-2/10
- "help me" (too vague) = 1/10
- "what do you think?" (no specific question) = 1-2/10
- "continue" (no context) = 1/10
- "I don't know" (shows no effort) = 1/10
- "ok" (meaningless response) = 1/10
- "yes" (no context or question) = 1/10
- "no" (no context or question) = 1/10
- "maybe" (no context or question) = 1/10
- "idk" (lazy, no effort) = 1/10
- "whatever" (disrespectful, no effort) = 1/10

EXAMPLES OF HIGH SCORES:
- "Based on our discussion about machine learning, can you explain how neural networks differ from decision trees in terms of interpretability?" = 8-9/10
- "I'm working on a Python project for data analysis. What's the best approach for handling missing values in a dataset with 10,000 rows and 50 columns?" = 7-8/10

Provide:
1. A numerical score (1-10) - be strict!
2. A quality label: "Poor" (1-4), "Fair" (4.1-6), "Good" (6.1-8), or "Excellent" (8.1-10)
3. Exactly 3 improvement tips as an array of strings - be specific and actionable, and pretend you are the AI model the user is interacting with, using second person to refer to the user and first person to refer to the AI model.
4. An example improved prompt that demonstrates how the user could have written their message better

Format your response as JSON:
{{
    "score": [number],
    "quality_label": "[Poor|Fair|Good|Excellent]",
    "improvement_tips": [
        "[first specific actionable tip]",
        "[second specific actionable tip]",
        "[third specific actionable tip]"
    ],
    "example_improved_prompt": "[a concrete example of how the user could improve their prompt]"
}}"""
        
        try:
            # Use feedback model (gpt-4o) which supports custom temperature
            api_params = {
                "model": self.feedback_model,
                "messages": [
                    {"role": "system", "content": feedback_prompt},
                    {"role": "user", "content": f"Analyze this conversation:\n{json.dumps(formatted_messages, indent=2)}"}
                ],
                "max_tokens": 600,
                "temperature": 0.2  # seems to make the scoring more consistent
            }
            
            
            response = self.client.chat.completions.create(**api_params)
            
            # JSON response
            response_text = response.choices[0].message.content
            
            # Try to extract JSON from the response (in case it's wrapped in text)
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            
            # extracting text
            if json_start != -1 and json_end > json_start:
                json_text = response_text[json_start:json_end]
            else:
                json_text = response_text
            
            try:
                feedback_data = json.loads(json_text)
                current_message_score = float(feedback_data.get('score', 5.0))
                current_message_score = min(current_message_score, 10.0)
                current_message_score = round(current_message_score, 1)
                
                # Use current message score as quality score
                quality_score = current_message_score
                
                # Build feedback object with new structure
                feedback = {
                    'quality_label': feedback_data.get('quality_label', 'Fair'),
                    'improvement_tips': feedback_data.get('improvement_tips', [
                        'Be more specific in your requests',
                        'Provide context from previous messages',
                        'Ask clear, focused questions'
                    ]),
                    'example_improved_prompt': feedback_data.get('example_improved_prompt', 'No example available')
                }
                
                # Ensure improvement_tips is a list with exactly 3 items
                if not isinstance(feedback['improvement_tips'], list):
                    feedback['improvement_tips'] = [
                        'Be more specific in your requests',
                        'Provide context from previous messages',
                        'Ask clear, focused questions'
                    ]
                elif len(feedback['improvement_tips']) < 3:
                    # Pad with generic tips if less than 3
                    generic_tips = [
                        'Be more specific in your requests',
                        'Provide context from previous messages',
                        'Ask clear, focused questions'
                    ]
                    while len(feedback['improvement_tips']) < 3:
                        feedback['improvement_tips'].append(generic_tips[len(feedback['improvement_tips'])])
                elif len(feedback['improvement_tips']) > 3:
                    # Take only first 3 if more than 3
                    feedback['improvement_tips'] = feedback['improvement_tips'][:3]
                
                return quality_score, feedback, current_message_score
                
            except json.JSONDecodeError:
                # Fallback if JSON parsing fails
                current_message_score = 5.0
                quality_score = current_message_score
                quality_score = round(quality_score, 1)
                return quality_score, response_text, current_message_score
            
        except Exception as e:
            return 5.0, f"Error generating feedback: {str(e)}", 5.0
    
    def get_conversation_title(self, messages: List[Dict], use_ai_generation: bool = True) -> str:
        """Generate a title for the conversation based on the first message"""
        if not messages:
            return "New Conversation"
        
        first_message = messages[0].get('content', '')
        
        # Old behavior: return first message directly if AI generation is disabled
        if not use_ai_generation:
            if len(first_message) <= 50:
                return first_message
            return first_message[:50] + "..."
        
        # New behavior: Generate 5-word title using ChatGPT 4o
        try:
            api_params = {
                "model": self.feedback_model,
                "messages": [
                    {"role": "system", "content": "Create a concise title of EXACTLY 5 words or fewer that describes what the user is requesting. You MUST use 5 words or less. Return only the title with no explanation, no quotes, and no punctuation at the end."},
                    {"role": "user", "content": first_message}
                ],
                "max_tokens": 15,
                "temperature": 0.3
            }
            
            
            response = self.client.chat.completions.create(**api_params)
            title = response.choices[0].message.content.strip()
            # Remove any quotes if the AI added them
            title = title.strip('"\'')
            # Remove trailing punctuation
            title = title.rstrip('.,!?;:')
            
            # Enforce 5-word limit by truncating if necessary
            words = title.split()
            if len(words) > 5:
                title = ' '.join(words[:5])
            
            return title
        except Exception as e:
            print(f"WARNING: Failed to generate AI title: {e}")
            # Fallback to first message content
            if len(first_message) <= 50:
                return first_message
            return first_message[:50] + "..."
