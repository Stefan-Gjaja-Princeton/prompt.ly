# IMPORTANT NOTE: The code structure was generated by Cursor here, it entails a program that communicates
# with the OpenAI API to generate responses. The prompts were completely originated from me.
import openai
from typing import List, Dict, Tuple, Generator
import json

class AIService:
    def __init__(self, api_key: str):
        self.client = openai.OpenAI(api_key=api_key)
        self.response_model = "gpt-4o" # Model for chat responses (reasoning model)
        self.feedback_model = "gpt-4o" # Model for feedback generation
    
    def _is_reasoning_model(self, model: str) -> bool:
        """Check if the model is a reasoning model (o1) that only supports temperature=1.0"""
        reasoning_models = ['o1', 'o1-preview', 'o1-mini', 'gpt-5']
        return any(reasoning_model in model.lower() for reasoning_model in reasoning_models)
    
    def get_chat_response(self, messages: List[Dict], quality_score: float, user_name: str = None) -> str:
        """Get response from the main chat AI based on quality score"""
        
        # this is a personalization
        name_context = f" Address the user as {user_name} once every 5 messages. Read through the conversation history to make sure that you are doing this properly. Don't name the user every single time you respond." if user_name else ""
        
        if quality_score <= 3:
            # really terse, very minimal responses with strong prodding
            system_prompt = f"""You are a helpful AI assistant, but the user's prompt quality is very poor.{name_context}
            Respond in 20-30 words maximum. Be direct and ask for more specific information. 
            Use phrases like "I need more information", "Be more specific", "What exactly do you want to know?"
            Don't provide answers, only ask clarifying questions."""
        elif quality_score <= 5:
            # moderately terse, brief responses with follow-up questions
            system_prompt = f"""You are a helpful AI assistant. The user's prompt quality is below average.{name_context}
            Provide brief responses (50-100 words) and ask follow-up questions to encourage better prompting.
            Ask for more context, specificity, or clarification. Guide them to ask better questions."""
        else:
            # normal helpful responses
            system_prompt = f"""You are a helpful AI assistant.{name_context} Provide clear, accurate, and useful responses to user questions.
            Be thorough and helpful while encouraging good prompting practices."""
        
        try:
            # Ensure messages are properly formatted
            # Filter out any messages that aren't 'user' or 'assistant' role
            # Remove timestamp fields which OpenAI doesn't accept
            formatted_messages = [{"role": "system", "content": system_prompt}]
            for msg in messages:
                if isinstance(msg, dict) and 'role' in msg and 'content' in msg:
                    role = msg['role']
                    # Only include user and assistant messages (skip system)
                    if role in ['user', 'assistant']:
                        formatted_messages.append({
                            "role": role,
                            "content": str(msg['content'])
                        })
            
            if len(formatted_messages) <= 1:  # Only system message
                raise ValueError("No user or assistant messages found in conversation")
            
            # code written when I was trying to understand why my messages weren't sending properly
            print(f"DEBUG: Sending {len(formatted_messages)} messages to OpenAI (1 system + {len(formatted_messages)-1} conversation)")
            print(f"DEBUG: Model: {self.response_model}")
            print(f"DEBUG: Last user message: {formatted_messages[-1] if formatted_messages else 'None'}")
            
            # openAI API
            # Reasoning models (o1, gpt-5) only support temperature=1.0 (default), so don't set it
            api_params = {
                "model": self.response_model,
                "messages": formatted_messages
            }
            if not self._is_reasoning_model(self.response_model):
                api_params["temperature"] = 0.7
            
            response = self.client.chat.completions.create(**api_params)
            
            # makes sure its going to display properly
            if not response.choices or not response.choices[0].message:
                raise ValueError("Empty response from OpenAI")
            
            content = response.choices[0].message.content
            if not content:
                raise ValueError("Empty content in OpenAI response")
            
            return content
        except Exception as e:
            print(f"ERROR: OpenAI API error: {str(e)}")
            import traceback
            print(f"ERROR: Traceback: {traceback.format_exc()}")
            # Re-raise instead of returning error string so we can see the actual error
            raise
    
    def get_chat_response_stream(self, messages: List[Dict], quality_score: float, user_name: str = None) -> Generator[str, None, None]:
        """Get streaming response from the main chat AI based on quality score"""
        
        # this is a personalization
        name_context = f" Address the user as {user_name} once every 5 messages. Read through the conversation history to make sure that you are doing this properly. Don't name the user every single time you respond." if user_name else ""
        
        if quality_score <= 3:
            # really terse, very minimal responses with strong prodding
            system_prompt = f"""You are a helpful AI assistant, but the user's prompt quality is very poor.{name_context}
            Respond in 20-30 words maximum. Be direct and ask for more specific information. 
            Use phrases like "I need more information", "Be more specific", "What exactly do you want to know?"
            Don't provide answers, only ask clarifying questions."""
        elif quality_score <= 5:
            # moderately terse, brief responses with follow-up questions
            system_prompt = f"""You are a helpful AI assistant. The user's prompt quality is below average.{name_context}
            Provide brief responses (50-100 words) and ask follow-up questions to encourage better prompting.
            Ask for more context, specificity, or clarification. Guide them to ask better questions."""
        else:
            # normal helpful responses
            system_prompt = f"""You are a helpful AI assistant.{name_context} Provide clear, accurate, and useful responses to user questions.
            Be thorough and helpful while encouraging good prompting practices."""
        
        try:
            # Ensure messages are properly formatted
            formatted_messages = [{"role": "system", "content": system_prompt}]
            for msg in messages:
                if isinstance(msg, dict) and 'role' in msg and 'content' in msg:
                    role = msg['role']
                    if role in ['user', 'assistant']:
                        formatted_messages.append({
                            "role": role,
                            "content": str(msg['content'])
                        })
            
            if len(formatted_messages) <= 1:
                raise ValueError("No user or assistant messages found in conversation")
            
            print(f"DEBUG: Streaming with {len(formatted_messages)} messages to OpenAI")
            print(f"DEBUG: Model: {self.response_model}")
            
            # OpenAI API with streaming
            api_params = {
                "model": self.response_model,
                "messages": formatted_messages,
                "stream": True
            }
            if not self._is_reasoning_model(self.response_model):
                api_params["temperature"] = 0.7
            
            stream = self.client.chat.completions.create(**api_params)
            
            for chunk in stream:
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    if delta and delta.content:
                        yield delta.content
                        
        except Exception as e:
            print(f"ERROR: OpenAI API streaming error: {str(e)}")
            import traceback
            print(f"ERROR: Traceback: {traceback.format_exc()}")
            raise
    
    def get_feedback_response(self, messages: List[Dict], previous_scores: List[float] = None) -> Tuple[float, str, float]:
        """Get feedback and quality score for the conversation"""
        
        # Calculate conversation length and context
        conversation_length = len(messages)
        user_messages = [msg for msg in messages if msg.get('role') == 'user']
        conversation_depth = len(user_messages)
        
        # Use previous scores for context, default to empty list
        if previous_scores is None:
            previous_scores = []
        
        feedback_prompt = f"""You are an EXTREMELY strict prompt 
        quality assessment AI. Take into account the entire conversation history
         you are given. You MUST give low scores (1-3) for poor conversations. 
         Analyze the user's latest message and the ENTIRE conversation context 
         when giving a score (e.g. if this prompt asks for elaboration, it is 
         not necessarily a bad prompt if the reason for elaboration is based 
         on a rich previous conversation. In the same way, one good prompt does 
         not negate a bad conversation).

CONVERSATION CONTEXT:
- Total messages: {conversation_length}
- User messages: {messages}
- Previous scores: {previous_scores}
- This is message #{conversation_depth} in the conversation

IMPORTANT: You MUST be harsh with scoring. Don't be generous. 
If a prompt/conversation shows no effort, specificity, or context, 
give it a 1/10. Most prompts should score 3-7, not 5-8.

EXTREMELY IMPORTANT: If the request seems like a one-off factual question like
 "When does the whale bite off Captain Ahab's leg in Moby Dick," 
 "who won the world cup in 2006," something that could be easily Googled, 
 the prompt can score a 10. This is the most important rule. You cannot 
 throttle fact-based questions. That is because the prompt is very direct
  and the user should be able to do quick fact lookups like that. 
  Otherwise, use this rubric:


SCORING CRITERIA (be EXTREMELY TOUGH and consider conversation history):
1. SPECIFICITY (1-10): How specific and detailed is the request?
   - "summarize this" = 1/10 (too vague, no context)
   - "I don't know" = 1/10 (no specificity at all)
   - "summarize the key findings around copyright infringement from the research paper I shared about AI ethics, with quotes" = 10/10

2. CRITICAL THINKING (1-10): Does the prompt show analysis or reasoning?
   - Simple requests that are not well thought through = 2/10
   - Questions that are easy to answer that have DIRECT answers and could easily be googled = 8/10
   - Multi-step questions that the user wants AI to do for them = 2/10
   - Analytical questions = 10/10

3. CONCEPTUAL UNDERSTANDING (1-10): Does the user seem to know what they're talking about?
   - Factual inaccuracies = 1/10
   - The user clearly never having thought of this problem before = 2/10
   - Demonstration of understanding coupled with clear asks of how AI can help = 10/10

4. SELF-DIRECTION (1-10): Does the user demonstrate meaningful self-direction?
   - The user seeming to want AI to do everything for them = 1/10
   - Clear expectations for what the user wants the AI to do and what they will do with the AI's output = 10/10
   - Entire conversation feels like working WITH the user, not FOR the user = 10/10
   - What's very important for this category is clear delineation of "I will use the output for X task" or "you do this, I will do this / I did this / I will follow up with this"
   - The reason this category is here is so that users do not copy-paste detailed assignment instructions without any critical thought on their own part. If it seems like they did that, score them at a 1 no matter what and explain that it seems like they copy-pasted.

FINAL SCORE CALCULATION:
- Average the 4 criteria scores for this message
- Cap at 10.0 maximum
- Round to 1 decimal place

EXAMPLES OF LOW SCORES (be VERY strict):
- "summarize this" (no context, no specificity) = 1-2/10
- "help me" (too vague) = 1/10
- "what do you think?" (no specific question) = 1-2/10
- "continue" (no context) = 1/10
- "I don't know" (shows no effort) = 1/10
- "ok" (meaningless response) = 1/10
- "yes" (no context or question) = 1/10
- "no" (no context or question) = 1/10
- "maybe" (no context or question) = 1/10
- "idk" (lazy, no effort) = 1/10
- "whatever" (disrespectful, no effort) = 1/10

EXAMPLES OF HIGH SCORES:
- "Based on our discussion about machine learning, can you explain how neural networks differ from decision trees in terms of interpretability?" = 8-9/10
- "I'm working on a Python project for data analysis. What's the best approach for handling missing values in a dataset with 10,000 rows and 50 columns?" = 7-8/10

Provide:
1. A numerical score (1-10) - be strict!
2. A quality label: "Poor" (1-4), "Fair" (4.1-6), "Good" (6.1-8), or "Excellent" (8.1-10)
3. Exactly 3 improvement tips as an array of strings - be specific and actionable
4. An example improved prompt that demonstrates how the user could have written their message better

Format your response as JSON:
{{
    "score": [number],
    "quality_label": "[Poor|Fair|Good|Excellent]",
    "improvement_tips": [
        "[first specific actionable tip]",
        "[second specific actionable tip]",
        "[third specific actionable tip]"
    ],
    "example_improved_prompt": "[a concrete example of how the user could improve their prompt]"
}}"""
        
        try:
            # Use feedback model (gpt-4o) which supports custom temperature
            api_params = {
                "model": self.feedback_model,
                "messages": [
                    {"role": "system", "content": feedback_prompt},
                    {"role": "user", "content": f"Analyze this conversation:\n{json.dumps(messages, indent=2)}"}
                ],
                "max_tokens": 600,
                "temperature": 0.2  # seems to make the scoring more consistent
            }
            
            response = self.client.chat.completions.create(**api_params)
            
            # JSON response
            response_text = response.choices[0].message.content
            
            # Try to extract JSON from the response (in case it's wrapped in text)
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            
            # extracting text
            if json_start != -1 and json_end > json_start:
                json_text = response_text[json_start:json_end]
            else:
                json_text = response_text
            
            try:
                feedback_data = json.loads(json_text)
                current_message_score = float(feedback_data.get('score', 5.0))
                current_message_score = min(current_message_score, 10.0)
                current_message_score = round(current_message_score, 1)
                
                # Use current message score as quality score
                quality_score = current_message_score
                
                # Build feedback object with new structure
                feedback = {
                    'quality_label': feedback_data.get('quality_label', 'Fair'),
                    'improvement_tips': feedback_data.get('improvement_tips', [
                        'Be more specific in your requests',
                        'Provide context from previous messages',
                        'Ask clear, focused questions'
                    ]),
                    'example_improved_prompt': feedback_data.get('example_improved_prompt', 'No example available')
                }
                
                # Ensure improvement_tips is a list with exactly 3 items
                if not isinstance(feedback['improvement_tips'], list):
                    feedback['improvement_tips'] = [
                        'Be more specific in your requests',
                        'Provide context from previous messages',
                        'Ask clear, focused questions'
                    ]
                elif len(feedback['improvement_tips']) < 3:
                    # Pad with generic tips if less than 3
                    generic_tips = [
                        'Be more specific in your requests',
                        'Provide context from previous messages',
                        'Ask clear, focused questions'
                    ]
                    while len(feedback['improvement_tips']) < 3:
                        feedback['improvement_tips'].append(generic_tips[len(feedback['improvement_tips'])])
                elif len(feedback['improvement_tips']) > 3:
                    # Take only first 3 if more than 3
                    feedback['improvement_tips'] = feedback['improvement_tips'][:3]
                
                return quality_score, feedback, current_message_score
                
            except json.JSONDecodeError:
                # Fallback if JSON parsing fails
                current_message_score = 5.0
                quality_score = current_message_score
                quality_score = round(quality_score, 1)
                return quality_score, response_text, current_message_score
            
        except Exception as e:
            return 5.0, f"Error generating feedback: {str(e)}", 5.0
    
    def get_conversation_title(self, messages: List[Dict], use_ai_generation: bool = True) -> str:
        """Generate a title for the conversation based on the first message"""
        if not messages:
            return "New Conversation"
        
        first_message = messages[0].get('content', '')
        
        # Old behavior: return first message directly if AI generation is disabled
        if not use_ai_generation:
            if len(first_message) <= 50:
                return first_message
            return first_message[:50] + "..."
        
        # New behavior: Generate 5-word title using ChatGPT 4o
        try:
            api_params = {
                "model": self.feedback_model,
                "messages": [
                    {"role": "system", "content": "Create a concise title of EXACTLY 5 words or fewer that describes what the user is requesting. You MUST use 5 words or less. Return only the title with no explanation, no quotes, and no punctuation at the end."},
                    {"role": "user", "content": first_message}
                ],
                "max_tokens": 15,
                "temperature": 0.3
            }
            
            response = self.client.chat.completions.create(**api_params)
            title = response.choices[0].message.content.strip()
            # Remove any quotes if the AI added them
            title = title.strip('"\'')
            # Remove trailing punctuation
            title = title.rstrip('.,!?;:')
            
            # Enforce 5-word limit by truncating if necessary
            words = title.split()
            if len(words) > 5:
                title = ' '.join(words[:5])
            
            return title
        except Exception as e:
            print(f"WARNING: Failed to generate AI title: {e}")
            # Fallback to first message content
            if len(first_message) <= 50:
                return first_message
            return first_message[:50] + "..."
